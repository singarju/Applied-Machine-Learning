# Applied-Machine-Learning
In this problem, you will apply different classification methods. You will use a Rock dataset where you will use 11 different rock features to predict the rock category. The data you need are included in these two files: 1) aggregateRockData.xlsx Download aggregateRockData.xlsxyou will only use 2nd column that contains the rock category number (1 = Igneous, 2 = Metamorphic, 3 = Sedimentary) - that will be the label. 2) features_presence540.txt Download features_presence540.txt, you will only use columns 4 to 14 as the attributes (features) and column 3 (token number) to separate train, validation and test data (see below). See this website for a detailed description of the dataset: https://osf.io/cvwu9/wiki?wiki=7ky6hLinks to an external site.We will use only the first 480 rows (so ignore rows 481 to 720). 

Answer the questions below directly in your Jupyter Notebook, using Markdown cells. Be sure to clearly indicate that your comment is an answer to a particular question.

Display the statistical values for each of the attributes, along with visualizations (e.g., histogram) of the distributions for each attribute. Are there any attributes that might require special treatment? If so, what special treatment might they require? [2 points]
Analyze and discuss the relationships between the data attributes and between the data attributes and labels. This involves computing the Pearson Correlation Coefficient (PCC) and generating scatter plots. [3 points]
For training data, use token numbers 7-16, for validation 4 to 6, and for testing 1 to 3 (each of the 30 rock subtypes has 16 token numbers). [2 points]
Train different classifiers and tweak the hyperparameters to improve performance (you can use the grid search if you want or manually try different values). Report training, validation and testing performance (classification accuracy, precision, recall and F1 score) and discuss the impact of the hyperparameters (use markdown cells in Jupyter Notebook to clearly indicate each solution):
Multinomial Logistic Regression (Softmax Regression); hyperparameters to explore: C, solver, max number of iterations. [10 points]
Support Vector Machine (make sure to try using kernels); hyperparameters to explore: C, kernel, degree of polynomial kernel, gamma.  [10 points]
Random Forest classifier (also analyze feature importance); hyperparameters to explore: the number of trees, max depth, the minimum number of samples required to split an internal node, the minimum number of samples required to be at a leaf node. [10 points]
Combine your classifiers into an ensemble and try to outperform each individual classifier on the validation set. Once you have found a good one, try it on the test set. Describe and discuss your findings. [8 points]
Is your method better than a human? Test that by taking human data from trialData.csv Download trialData.csv(see hereLinks to an external site. for a description of the file). Compute human accuracy on train and test rocks jointly (use only rocks with numbers 1 to 480 and Block number 4 (blocks 1-3 were used for human training, ignore those), use column cat_correct for human categorization accuracy). How does the human accuracy compare to the accuracy of your best model for each model type (softmax regression, SVM, Random Forest, and Ensemble)? [2 points]. Make a plot with the x-axis showing average human accuracy (values between 0 and 1) and y-axis showing model probability (also values between 0 and 1) for 480 rocks (regardless of whether they were used for train or test). Each rock should be represented with a dot in this plot. [2 points] Compute the correlation coefficient between average human accuracies and model probabilities of the correct class for your best model for each model type (softmax regression, SVM, Random Forest, and Ensemble) for all 480 rocks (so you need to compute four correlation coefficients). Report the p-value for each of the four correlation coefficients. Are the correlations significant? [1 point]
